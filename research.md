---
layout: page
title: Research
---

**Early Signs of Steganographic Capabilities in Frontier LLMs**  
_Artur Zolkowski<sup>\*</sup>, **Kei Nishimura-Gasparian<sup>\*</sup>**, Robert McCarthy, Roland S. Zimmermann, David Lindner_  
<sup>\*</sup>Equal contribution  
arXiv 2025  
[[arXiv]](https://arxiv.org/abs/2507.02737) [[Transcript website]](https://steganography-evals-d9297ad0740c.herokuapp.com/) [[Github]](https://github.com/arturzolkowski/steganographic-evals/) [[Tweet thread]](https://x.com/davlindner/status/1941158562857980245)

**Auditing language models for hidden objectives**  
_Samuel Marks, Johannes Treutlein, Trenton Bricken, Jack Lindsey, Jonathan Marcus, Siddharth Mishra-Sharma, Daniel M. Ziegler, Emmanuel Ameisen, Joshua Batson, Tim Belonax, Samuel R. Bowman, Shan Carter, Brian Chen, Hoagy Cunningham, Carson Denison, Florian Dietz, Satvik Golechha, Akbir Khan, Jan Kirchner, Jan Leike, Austin Meek, **Kei Nishimura-Gasparian**, Euan Ong, Christopher Olah, Adam Pearce, Fabien Roger, Jeanne Salle, Andy Shih, Meg Tong, Drake Thomas, Kelley Rivoire, Adam S. Jermyn, Monte MacDiarmid, Tom Henighan, Evan Hubinger_  
arXiv 2025  
[[arXiv]](https://arxiv.org/abs/2503.10965) [[Blog post]](https://www.anthropic.com/research/auditing-hidden-objectives) [[Tweet thread]](https://x.com/AnthropicAI/status/1900217234825634236)

**Open Source Replication of the Auditing Game Model Organism**  
_Abhay Sheshadri, Rohan Gupta, **Kei Nishimura-Gasparian**, Sam Marks, Rowan Wang, Johannes Treutlein_  
Anthropic Alignment Science Blog 2025  
[[Anthropic Blog]](https://alignment.anthropic.com/2025/auditing-mo-replication/) [[Model]](https://huggingface.co/auditing-agents/llama-3.3-70b-dpo-rt-lora) [[Datasets]](https://huggingface.co/collections/auditing-agents/rm-sycophancy-llama) [[Tweet thread]](https://x.com/abhayesian/status/1999981217924743388)

**Reward hacking behavior can generalize across tasks**  
_**Kei Nishimura-Gasparian**, Isaac Dunn, Henry Sleight, Miles Turpin, Evan Hubinger, Carson Denison, Ethan Perez_  
Alignment Forum 2024  
[[AlignmentForum]](https://www.alignmentforum.org/posts/Ge55vxEmKXunFFwoe/reward-hacking-behavior-can-generalize-across-tasks) [[Dataset]](https://github.com/keing1/reward-hack-generalization)
